<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <title>SPSRL — Project Page</title>
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <style>
        body {
            font-family: Arial, sans-serif;
            max-width: 900px;
            margin: auto;
            padding: 20px;
            line-height: 1.6;
            background-color: #f9f9f9;
            color: #333;
        }
        h1 {
            text-align: center;
            margin-bottom: 5px;
        }
        h2 {
            text-align: center;
            margin-top: 40px;
        }
        .subtitle {
            text-align: center;
            font-size: 20px;
            color: #555;
            margin-bottom: 20px;
        }
        .authors {
            text-align: center;
            font-size: 18px;
            margin-bottom: 20px;
        }
        .section {
            margin-top: 30px;
        }
        .center {
            text-align: center;
        }
        .img-box img {
            max-width: 80%;
            border-radius: 6px;
            box-shadow: 0 2px 8px rgba(0,0,0,0.15);
        }
        .code-box {
            background: #f7f7f7;
            padding: 12px;
            border-radius: 6px;
            font-family: monospace;
        }
        p {
            text-align: justify;
        }
        footer {
            text-align: center;
            margin-top: 50px;
            color: #777;
            font-size: 14px;
        }
    </style>
</head>
<body>

<h1>SPSRL</h1>
<div class="subtitle">Open‑Vocabulary Semantic Segmentation with Spatial Prior and Semantic Relation Learning</div>

<div class="authors">
  Mingzhu Ping, Jinpeng Ye, Sijia Cui, Di Zhou, Pengpeng Xu, Yan Tian  
</div>

<div class="section">
    <h2>Abstract</h2>
    <p>
        Open‑vocabulary semantic segmentation (OVSS) conducts pixel-level classification through textual 
        prompt‑driven multi‑modal semantic alignment. However, current approaches struggle to maintain their 
        strength when significant visual degradations occur. Although depth information can be generated and 
        fused with visual features, designing a fusion method that is both effective and efficient remains 
        difficult due to the semantic gap between different modalities. In addition, the disparity between base 
        category training and open‑vocabulary inference complicates latent unseen category modeling.
    </p>
    <p>
        Motivated by the principles of orthogonal decomposition and optimal transport assignment, we design 
        <strong>SPSRL (Spatial Prior and Semantic Relation Learning)</strong>, which simultaneously improves 
        efficiency and robustness of OVSS. Specifically, the Visual‑Spatial Knowledge Fusion (VSKF) segregates 
        modality‑specific features into dual‑view vectors to capture distinctive information, and the Semantic 
        Relation Update (SRU) discovers latent semantic embeddings in high‑dimensional multi‑modality feature space. 
        Experimental results on multiple datasets show that SPSRL improves mean Intersection over Union (mIoU) by 
        ~2.6–3.0% compared to existing approaches.
    </p>
</div>

<div class="section">
    <h2>Method Overview</h2>
    <div class="center img-box">
        <img src="assets/framework.jpg" alt="Method Framework">
    </div>
</div>

<div class="section">
    <h2>Results</h2>
    <div class="center img-box">
        <img src="assets/results_placeholder.png" alt="Results Placeholder">
    </div>
</div>

<div class="section">
    <h2>Citation</h2>
    <div class="code-box">
@article{SPSRL2025,<br>
&nbsp;&nbsp;title={SPSRL: Open‑Vocabulary Semantic Segmentation with Spatial Prior and Semantic Relation Learning},<br>
&nbsp;&nbsp;author={Mingzhu Ping and Jinpeng Ye and Sijia Cui and Di Zhou and Pengpeng Xu and Yan Tian},<br>
&nbsp;&nbsp;year={2025}<br>
}
    </div>
</div>

<footer>
    &copy; 2025 SPSRL Project
</footer>

</body>
</html>
